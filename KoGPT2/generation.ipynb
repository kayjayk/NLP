{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")\n",
    "\n",
    "# input_ids = tokenizer.encode(\"안녕\", add_special_tokens=False, return_tensors=\"pt\")\n",
    "# output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n",
    "# for generated_sequence in output_sequences:\n",
    "#     generated_sequence = generated_sequence.tolist()\n",
    "#     print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50000, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"스마트폰 \"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=50, num_return_sequences=3)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 스마트폰 중독현상의 이유 중 하나는 스마트폰과 태블릿의 크기 차이 때문이라는 지적이 많다.</s><s> 박 대통령은 또 “제가 제시한 국정과제의 실천과 비전으로 삼겠다”며 “한강의 기적을 일으키는 데 큰 역할을 한 분들이 바로 박 대통령”\n",
      "GENERATED SEQUENCE : 스마트폰 중독현상의 이유 및 해결 방안에 관한 온라인 설문조사를 진행한 결과 부모들의 스마트폰 중독 현상의 중요한 원인으로는 `학습효과`(39.6%)로 압도적이었다.</s><s> ‘한·터키 전략적 협력관계 수립’과 ‘양국 교류 활성화’, ‘\n",
      "GENERATED SEQUENCE : 스마트폰 중독현상의 이유 중 하나는 자녀 때문에 부모들 스스로가 '가난하다'와 '열등감'을 느끼고 있다는 점이다.</s><s> 하지만 일각에서는 지난 14일 국회 미래창조과학방송통신위원회가 주최한 '방송정책 및 방송통신발전 특별위원회'에서 \"\n"
     ]
    }
   ],
   "source": [
    "input_str = \"스마트폰 중독현상의 이유\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=50, num_return_sequences=3)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 회식만 하면 술주정하는 걸 보고, 정말 어처구니가 없는 모양입니다.</s>\n"
     ]
    }
   ],
   "source": [
    "input_str = \"회식\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 휴가 휴가를 떠나는 사람들을 대상으로 무료 숙박도 제공하는 ‘여름 바캉스 캠프’를 마련했다.</s><s> 또한\n"
     ]
    }
   ],
   "source": [
    "input_str = \"휴가\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 출장 및 출국 시 여권, 인감 도장, 여권용 사본 등 관련 서류 일체를 갖고\n"
     ]
    }
   ],
   "source": [
    "input_str = \"출장\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 연봉 1억원을 돌파해 '최다 연봉'에도 올랐다.</s><s> 이어 이들은 “이번 결정은\n"
     ]
    }
   ],
   "source": [
    "input_str = \"연봉\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 조직에서 은퇴 후 가족과 함께 살고 싶어한다는 얘기를 들은 적이 있다.</s><s> 특히, 이번 교육은\n"
     ]
    }
   ],
   "source": [
    "input_str = \"조직\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 성과 성과 평가체계를 구축했다\"며 \"실태를 정확히 파악해 이를 바탕으로 성과평가체계를 개편할 것\n"
     ]
    }
   ],
   "source": [
    "input_str = \"성과\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SEQUENCE : 커리어가 없는 한 쉽게 성공할 수 없을 것이라고 전망했다.</s><s> 앞서 지난 23일 오후 1시께\n"
     ]
    }
   ],
   "source": [
    "input_str = \"커리어\"\n",
    "input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors=\"pt\")\n",
    "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=20, num_return_sequences=1)\n",
    "for generated_sequence in output_sequences:\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
